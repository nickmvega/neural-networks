{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering Data + Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEED VAE IMPLEMENTATION FROM THIS THATS ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Coordinate shape: torch.Size([784, 34])\n",
      "Coordinate min/max: -1.0 1.0\n",
      "Encoded coordinate shape: torch.Size([784, 34])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 2/3750 [00:00<05:07, 12.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/3750: Loss: 0.161701, Recon: 0.161701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   3%|▎         | 102/3750 [00:08<05:18, 11.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100/3750: Loss: 0.150019, Recon: 0.150019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   5%|▌         | 203/3750 [00:16<03:10, 18.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200/3750: Loss: 0.173555, Recon: 0.173555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   8%|▊         | 304/3750 [00:21<02:48, 20.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300/3750: Loss: 0.176901, Recon: 0.176901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  11%|█         | 403/3750 [00:26<02:39, 21.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400/3750: Loss: 0.158308, Recon: 0.158308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  13%|█▎        | 505/3750 [00:31<02:34, 20.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500/3750: Loss: 0.157883, Recon: 0.157883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  16%|█▌        | 604/3750 [00:35<02:37, 20.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 600/3750: Loss: 0.156225, Recon: 0.156225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  19%|█▊        | 702/3750 [00:40<02:30, 20.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 700/3750: Loss: 0.160167, Recon: 0.160167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  21%|██▏       | 805/3750 [00:45<02:36, 18.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 800/3750: Loss: 0.153816, Recon: 0.153816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  24%|██▍       | 904/3750 [00:52<02:57, 16.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 900/3750: Loss: 0.168452, Recon: 0.168452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  27%|██▋       | 1003/3750 [00:57<02:09, 21.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/3750: Loss: 0.163996, Recon: 0.163996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  29%|██▉       | 1104/3750 [01:03<02:35, 16.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1100/3750: Loss: 0.169841, Recon: 0.169841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  32%|███▏      | 1203/3750 [01:09<02:26, 17.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1200/3750: Loss: 0.161970, Recon: 0.161970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  35%|███▍      | 1303/3750 [01:14<02:17, 17.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1300/3750: Loss: 0.165253, Recon: 0.165253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  37%|███▋      | 1405/3750 [01:19<01:49, 21.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1400/3750: Loss: 0.171792, Recon: 0.171792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  40%|████      | 1504/3750 [01:24<01:47, 20.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500/3750: Loss: 0.156772, Recon: 0.156772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  43%|████▎     | 1604/3750 [01:29<01:48, 19.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1600/3750: Loss: 0.151292, Recon: 0.151292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  45%|████▌     | 1703/3750 [01:35<01:49, 18.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1700/3750: Loss: 0.166345, Recon: 0.166345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  48%|████▊     | 1805/3750 [01:40<01:35, 20.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1800/3750: Loss: 0.166865, Recon: 0.166865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  51%|█████     | 1903/3750 [01:44<01:28, 20.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1900/3750: Loss: 0.174835, Recon: 0.174835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  53%|█████▎    | 2004/3750 [01:49<01:22, 21.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000/3750: Loss: 0.179913, Recon: 0.179913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  56%|█████▌    | 2105/3750 [01:54<01:21, 20.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2100/3750: Loss: 0.169694, Recon: 0.169694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  59%|█████▉    | 2205/3750 [01:59<01:14, 20.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2200/3750: Loss: 0.169520, Recon: 0.169520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  61%|██████▏   | 2304/3750 [02:04<01:09, 20.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2300/3750: Loss: 0.165718, Recon: 0.165718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  64%|██████▍   | 2403/3750 [02:09<01:05, 20.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2400/3750: Loss: 0.167273, Recon: 0.167273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  67%|██████▋   | 2505/3750 [02:14<00:58, 21.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2500/3750: Loss: 0.168533, Recon: 0.168533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  69%|██████▉   | 2604/3750 [02:18<00:54, 21.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2600/3750: Loss: 0.165044, Recon: 0.165044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  72%|███████▏  | 2703/3750 [02:23<00:49, 21.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2700/3750: Loss: 0.173134, Recon: 0.173134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  75%|███████▍  | 2805/3750 [02:28<00:47, 20.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2800/3750: Loss: 0.164641, Recon: 0.164641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  77%|███████▋  | 2903/3750 [02:33<00:41, 20.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2900/3750: Loss: 0.166172, Recon: 0.166172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  80%|████████  | 3004/3750 [02:38<00:35, 21.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3000/3750: Loss: 0.152275, Recon: 0.152275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  83%|████████▎ | 3103/3750 [02:43<00:30, 21.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3100/3750: Loss: 0.157655, Recon: 0.157655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  85%|████████▌ | 3205/3750 [02:48<00:25, 21.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3200/3750: Loss: 0.171302, Recon: 0.171302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  88%|████████▊ | 3304/3750 [02:53<00:21, 20.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3300/3750: Loss: 0.160283, Recon: 0.160283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  91%|█████████ | 3404/3750 [02:57<00:17, 20.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3400/3750: Loss: 0.173895, Recon: 0.173895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  93%|█████████▎| 3505/3750 [03:03<00:12, 20.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3500/3750: Loss: 0.167728, Recon: 0.167728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  96%|█████████▌| 3603/3750 [03:09<00:07, 20.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3600/3750: Loss: 0.170103, Recon: 0.170103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  99%|█████████▉| 3704/3750 [03:15<00:02, 20.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3700/3750: Loss: 0.167345, Recon: 0.167345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 3750/3750 [03:17<00:00, 18.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1] Loss: 0.164933, Recon: 0.164933, \n",
      "Training complete!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x600 with 24 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, transform=transform)\n",
    "test_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, train=False, transform=transform)\n",
    "\n",
    "batch_size = 16  \n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n",
    "\n",
    "def positional_encoding(coords, num_bands=8):\n",
    "    \"\"\"Apply positional encoding to coordinates with multiple frequency bands.\"\"\"\n",
    "    pos_enc = [coords]\n",
    "    for i in range(num_bands):\n",
    "        freq = 2.0 ** i\n",
    "        pos_enc.append(torch.sin(coords * freq * np.pi))\n",
    "        pos_enc.append(torch.cos(coords * freq * np.pi))\n",
    "    return torch.cat(pos_enc, dim=-1)\n",
    "\n",
    "def get_mgrid(sidelen, dim=2, num_bands=8):\n",
    "    '''Generates a flattened grid of (x,y,...) coordinates in a range of -1 to 1.'''\n",
    "    if isinstance(sidelen, int):\n",
    "        sidelen = dim * (sidelen,)\n",
    "    coords = [torch.linspace(-1, 1, s) for s in sidelen]\n",
    "    mesh_coords = torch.meshgrid(*coords, indexing='ij')  \n",
    "    coords = torch.stack(mesh_coords, dim=-1).reshape(-1, dim)\n",
    "    return positional_encoding(coords, num_bands)\n",
    "\n",
    "class SIREN(nn.Module):\n",
    "    def __init__(self, input_dim=2+2*2*8, hidden_dim=256, hidden_layers=3, output_dim=1, first_omega=1.0, hidden_omega=10.0):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.first_omega = first_omega\n",
    "        self.hidden_omega = hidden_omega\n",
    "\n",
    "        self.first_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        with torch.no_grad():\n",
    "\n",
    "            self.first_layer.weight.uniform_(-1/input_dim, 1/input_dim)\n",
    "            self.first_layer.bias.uniform_(-1/input_dim, 1/input_dim)\n",
    "        \n",
    "        self.hidden_layers_list = nn.ModuleList()\n",
    "        for _ in range(hidden_layers):\n",
    "            layer = nn.Linear(hidden_dim, hidden_dim)\n",
    "            with torch.no_grad():\n",
    "\n",
    "                layer.weight.uniform_(-np.sqrt(6/hidden_dim)/hidden_omega, \n",
    "                                      np.sqrt(6/hidden_dim)/hidden_omega)\n",
    "                layer.bias.uniform_(-np.sqrt(6/hidden_dim)/hidden_omega, \n",
    "                                    np.sqrt(6/hidden_dim)/hidden_omega)\n",
    "            self.hidden_layers_list.append(layer)\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        with torch.no_grad():\n",
    "\n",
    "            self.output_layer.weight.uniform_(-np.sqrt(6/hidden_dim)/hidden_omega, \n",
    "                                              np.sqrt(6/hidden_dim)/hidden_omega)\n",
    "            self.output_layer.bias.uniform_(-1, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.sin(self.first_omega * self.first_layer(x))\n",
    "        for layer in self.hidden_layers_list:\n",
    "            x = torch.sin(self.hidden_omega * layer(x))\n",
    "\n",
    "        return torch.sigmoid(self.output_layer(x))\n",
    "    \n",
    "    def set_weights_vector(self, weights_vector):\n",
    "        \"\"\"Set all weights from a flattened vector\"\"\"\n",
    "        start_idx = 0\n",
    "        \n",
    "\n",
    "        w_size = self.first_layer.weight.numel()\n",
    "        self.first_layer.weight.data = weights_vector[start_idx:start_idx+w_size].view(self.first_layer.weight.shape)\n",
    "        start_idx += w_size\n",
    "        \n",
    "        b_size = self.first_layer.bias.numel()\n",
    "        self.first_layer.bias.data = weights_vector[start_idx:start_idx+b_size].view(self.first_layer.bias.shape)\n",
    "        start_idx += b_size\n",
    "        \n",
    "\n",
    "        for layer in self.hidden_layers_list:\n",
    "            w_size = layer.weight.numel()\n",
    "            layer.weight.data = weights_vector[start_idx:start_idx+w_size].view(layer.weight.shape)\n",
    "            start_idx += w_size\n",
    "            \n",
    "            b_size = layer.bias.numel()\n",
    "            layer.bias.data = weights_vector[start_idx:start_idx+b_size].view(layer.bias.shape)\n",
    "            start_idx += b_size\n",
    "        \n",
    "\n",
    "        w_size = self.output_layer.weight.numel()\n",
    "        self.output_layer.weight.data = weights_vector[start_idx:start_idx+w_size].view(self.output_layer.weight.shape)\n",
    "        start_idx += w_size\n",
    "        \n",
    "        b_size = self.output_layer.bias.numel()\n",
    "        self.output_layer.bias.data = weights_vector[start_idx:start_idx+b_size].view(self.output_layer.bias.shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def calc_total_weights(input_dim, hidden_dim, hidden_layers, output_dim):\n",
    "        \"\"\"Calculate the total number of weights in the SIREN\"\"\"\n",
    "        total = input_dim * hidden_dim + hidden_dim\n",
    "        \n",
    "        total += hidden_layers * (hidden_dim * hidden_dim + hidden_dim)\n",
    "        \n",
    "        total += hidden_dim * output_dim + output_dim\n",
    "        \n",
    "        return total\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, z_dim=64, siren_config=None):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "        # Default SIREN config\n",
    "        self.siren_config = {\n",
    "            'input_dim': 2 + 2*2*8,  \n",
    "            'hidden_dim': 128,       \n",
    "            'hidden_layers': 3,       \n",
    "            'output_dim': 1\n",
    "        }\n",
    "        \n",
    "        # Update config if provided\n",
    "        if siren_config is not None:\n",
    "            self.siren_config.update(siren_config)\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 4, stride=2, padding=1),             # 28x28 -> 14x14\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(32, 64, 4, stride=2, padding=1),            # 14x14 -> 7x7\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 3, stride=1, padding=1),           # 7x7 -> 7x7\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 7 * 7, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.fc_mu = nn.Linear(512, z_dim)\n",
    "        self.fc_logvar = nn.Linear(512, z_dim)\n",
    "        \n",
    "\n",
    "        total_weights = SIREN.calc_total_weights(\n",
    "            input_dim=self.siren_config['input_dim'],\n",
    "            hidden_dim=self.siren_config['hidden_dim'],\n",
    "            hidden_layers=self.siren_config['hidden_layers'],\n",
    "            output_dim=self.siren_config['output_dim']\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, total_weights) \n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        logvar = torch.clamp(logvar, -4, 4)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    # Modify your VAE's decode method\n",
    "    def decode(self, z):\n",
    "        raw_weights = self.decoder(z)\n",
    "        \n",
    "        # Split and scale weights for different SIREN layers\n",
    "        weights_list = []\n",
    "        start_idx = 0\n",
    "        \n",
    "        # First layer weights - needs uniform(-1/in_dim, 1/in_dim)\n",
    "        in_dim = self.siren_config['input_dim']\n",
    "        first_w_size = in_dim * self.siren_config['hidden_dim']\n",
    "        first_b_size = self.siren_config['hidden_dim']\n",
    "        \n",
    "        first_w = raw_weights[:, start_idx:start_idx+first_w_size].view(-1, self.siren_config['hidden_dim'], in_dim)\n",
    "        start_idx += first_w_size\n",
    "        first_w = torch.tanh(first_w) * (1.0/in_dim)  # Scale to proper range\n",
    "        weights_list.append(first_w.reshape(-1, first_w_size))\n",
    "        \n",
    "        first_b = raw_weights[:, start_idx:start_idx+first_b_size]\n",
    "        start_idx += first_b_size\n",
    "        first_b = torch.tanh(first_b) * (1.0/in_dim)  # Scale to proper range\n",
    "        weights_list.append(first_b)\n",
    "        \n",
    "        # Hidden layers - needs scaled by sqrt(6/hidden_dim)/omega\n",
    "        hidden_dim = self.siren_config['hidden_dim']\n",
    "        hidden_omega = 30.0\n",
    "        scale_factor = np.sqrt(6/hidden_dim)/hidden_omega\n",
    "        \n",
    "        for _ in range(self.siren_config['hidden_layers']):\n",
    "            hidden_w_size = hidden_dim * hidden_dim\n",
    "            hidden_w = raw_weights[:, start_idx:start_idx+hidden_w_size].view(-1, hidden_dim, hidden_dim)\n",
    "            start_idx += hidden_w_size\n",
    "            hidden_w = torch.tanh(hidden_w) * scale_factor\n",
    "            weights_list.append(hidden_w.reshape(-1, hidden_w_size))\n",
    "            \n",
    "            hidden_b_size = hidden_dim\n",
    "            hidden_b = raw_weights[:, start_idx:start_idx+hidden_b_size]\n",
    "            start_idx += hidden_b_size\n",
    "            hidden_b = torch.tanh(hidden_b) * scale_factor\n",
    "            weights_list.append(hidden_b)\n",
    "        \n",
    "        # Output layer\n",
    "        out_dim = self.siren_config['output_dim']\n",
    "        out_w_size = hidden_dim * out_dim\n",
    "        out_w = raw_weights[:, start_idx:start_idx+out_w_size].view(-1, out_dim, hidden_dim)\n",
    "        start_idx += out_w_size\n",
    "        out_w = torch.tanh(out_w) * scale_factor\n",
    "        weights_list.append(out_w.reshape(-1, out_w_size))\n",
    "        \n",
    "        out_b_size = out_dim\n",
    "        out_b = raw_weights[:, start_idx:start_idx+out_b_size]\n",
    "        out_b = torch.tanh(out_b)  # Range (-1, 1) is fine for output bias\n",
    "        weights_list.append(out_b)\n",
    "        \n",
    "        # Concatenate all scaled weights\n",
    "        return torch.cat(weights_list, dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "def train(num_epochs=1, save_interval=1):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "\n",
    "    num_bands = 8\n",
    "    siren_config = {\n",
    "        'input_dim': 2 + 2*2*num_bands,\n",
    "        'hidden_dim': 128, \n",
    "        'hidden_layers': 3, \n",
    "        'output_dim': 1,\n",
    "    }\n",
    "    \n",
    "\n",
    "    vae = VAE(z_dim=64, siren_config=siren_config).to(device) \n",
    "    siren = SIREN(\n",
    "        input_dim=siren_config['input_dim'],\n",
    "        hidden_dim=siren_config['hidden_dim'],\n",
    "        hidden_layers=siren_config['hidden_layers'],\n",
    "        output_dim=siren_config['output_dim']\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(vae.parameters(), lr=0.0005)\n",
    "    \n",
    "    coords = get_mgrid(28, num_bands=num_bands).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(\"Coordinate shape:\", coords.shape)\n",
    "        print(\"Coordinate min/max:\", coords[:, 0].min().item(), coords[:, 0].max().item())\n",
    "        print(\"Encoded coordinate shape:\", coords.shape)\n",
    "        \n",
    "        # Visualize the coordinate grid to make sure it's correct\n",
    "        coord_grid = coords[:, :2].reshape(28, 28, 2)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(coord_grid[:, :, 0].cpu())\n",
    "        plt.title(\"X coordinates\")\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(coord_grid[:, :, 1].cpu())\n",
    "        plt.title(\"Y coordinates\")\n",
    "        plt.savefig(\"coordinate_grid.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    example_data = next(iter(test_loader))\n",
    "    example_images = example_data[0][:8].to(device)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        vae.train()\n",
    "        total_loss = 0\n",
    "        total_recon_loss = 0\n",
    "        total_kl_loss = 0\n",
    "        \n",
    "        for batch_idx, (images, _) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "            images = images.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            siren_weights_batch, mu, logvar = vae(images)\n",
    "            \n",
    "            reconstruction_loss = 0\n",
    "            \n",
    "            reconstructed_images = []\n",
    "            for j in range(batch_size):\n",
    "                siren.set_weights_vector(siren_weights_batch[j])\n",
    "                \n",
    "                reconstructed_image = siren(coords).reshape(1, 1, 28, 28)\n",
    "                reconstructed_images.append(reconstructed_image)\n",
    "            \n",
    "            reconstructed_batch = torch.cat(reconstructed_images, dim=0)\n",
    "            \n",
    "            reconstruction_loss = F.mse_loss(reconstructed_batch, images)\n",
    "            \n",
    "            loss = reconstruction_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(vae.parameters(), max_norm=1.0)\n",
    "            \n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_recon_loss += reconstruction_loss.item()\n",
    "            \n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Batch {batch_idx}/{len(train_loader)}: \"\n",
    "                      f\"Loss: {loss.item():.6f}, \"\n",
    "                      f\"Recon: {reconstruction_loss.item():.6f}\")\n",
    "        \n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_recon = total_recon_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "              f\"Loss: {avg_loss:.6f}, \"\n",
    "              f\"Recon: {avg_recon:.6f}, \")\n",
    "        \n",
    "\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "\n",
    "            torch.save({\n",
    "                'vae_state_dict': vae.state_dict(),\n",
    "                'siren_config': siren_config,\n",
    "                'epoch': epoch,\n",
    "                'loss': avg_loss\n",
    "            }, 'best_vae_siren_model.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        if epoch % save_interval == 0:\n",
    "            with torch.no_grad():\n",
    "                vae.eval()\n",
    "                \n",
    "\n",
    "                test_recon_images = []\n",
    "                for i in range(example_images.size(0)):\n",
    "                    # Encode and decode\n",
    "                    mu, logvar = vae.encode(example_images[i:i+1])\n",
    "                    z = vae.reparameterize(mu, logvar)\n",
    "                    siren_weights = vae.decode(z)\n",
    "                    \n",
    "                    # Set weights and get reconstruction\n",
    "                    siren.set_weights_vector(siren_weights.squeeze(0))\n",
    "                    recon_image = siren(coords).reshape(1, 28, 28)\n",
    "                    test_recon_images.append(recon_image)\n",
    "                \n",
    "                # Stack the images\n",
    "                test_recon_images = torch.cat(test_recon_images, dim=0)\n",
    "                \n",
    "                # Generate random samples\n",
    "                z = torch.randn(8, vae.z_dim, device=device)\n",
    "                generated_images = []\n",
    "                for i in range(z.size(0)):\n",
    "                    siren_weights = vae.decode(z[i:i+1])\n",
    "                    siren.set_weights_vector(siren_weights.squeeze(0))\n",
    "                    gen_image = siren(coords).reshape(1, 28, 28)\n",
    "                    generated_images.append(gen_image)\n",
    "                \n",
    "\n",
    "                generated_images = torch.cat(generated_images, dim=0)\n",
    "                \n",
    "\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                \n",
    "\n",
    "                for i in range(8):\n",
    "                    plt.subplot(3, 8, i + 1)\n",
    "                    plt.imshow(example_images[i].cpu().squeeze(), cmap='gray')\n",
    "                    plt.axis('off')\n",
    "                    plt.title(\"Original\")\n",
    "                \n",
    "                for i in range(8):\n",
    "                    plt.subplot(3, 8, i + 9)\n",
    "                    plt.imshow(test_recon_images[i].cpu().squeeze(), cmap='gray')\n",
    "                    plt.axis('off')\n",
    "                    plt.title(\"Reconstructed\")\n",
    "                \n",
    "                # Plot generated images\n",
    "                for i in range(8):\n",
    "                    plt.subplot(3, 8, i + 17)\n",
    "                    plt.imshow(generated_images[i].cpu().squeeze(), cmap='gray')\n",
    "                    plt.axis('off')\n",
    "                    plt.title(\"Generated\")\n",
    "                \n",
    "                plt.suptitle(f\"Epoch {epoch+1}\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"vae_siren_epoch_{epoch+1}.png\")\n",
    "                plt.close()\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    return vae, siren\n",
    "\n",
    "def evaluate_model(model_path='best_vae_siren_model.pt'):\n",
    "    # Load model\n",
    "    checkpoint = torch.load(model_path)\n",
    "    siren_config = checkpoint['siren_config']\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    vae = VAE(z_dim=64, siren_config=siren_config).to(device)\n",
    "    vae.load_state_dict(checkpoint['vae_state_dict'])\n",
    "    \n",
    "    siren = SIREN(\n",
    "        input_dim=siren_config['input_dim'],\n",
    "        hidden_dim=siren_config['hidden_dim'],\n",
    "        hidden_layers=siren_config['hidden_layers'],\n",
    "        output_dim=siren_config['output_dim']\n",
    "    ).to(device)\n",
    "    \n",
    "    coords = get_mgrid(28, num_bands=8).to(device)\n",
    "    \n",
    "    example_data = next(iter(test_loader))\n",
    "    test_images = example_data[0][:8].to(device)\n",
    "    test_labels = example_data[1][:8]\n",
    "    \n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        reconstructed_images = []\n",
    "        for i in range(test_images.size(0)):\n",
    "            mu, logvar = vae.encode(test_images[i:i+1])\n",
    "            z = vae.reparameterize(mu, logvar)\n",
    "            siren_weights = vae.decode(z)\n",
    "            \n",
    "            siren.set_weights_vector(siren_weights.squeeze(0))\n",
    "            recon_image = siren(coords).reshape(1, 28, 28)\n",
    "            reconstructed_images.append(recon_image)\n",
    "        \n",
    "        reconstructed_images = torch.cat(reconstructed_images, dim=0)\n",
    "        \n",
    "        z = torch.randn(8, vae.z_dim, device=device)\n",
    "        generated_images = []\n",
    "        for i in range(z.size(0)):\n",
    "            siren_weights = vae.decode(z[i:i+1])\n",
    "            siren.set_weights_vector(siren_weights.squeeze(0))\n",
    "            gen_image = siren(coords).reshape(1, 28, 28)\n",
    "            generated_images.append(gen_image)\n",
    "        \n",
    "        generated_images = torch.cat(generated_images, dim=0)\n",
    "    \n",
    "    fig, axs = plt.subplots(3, 8, figsize=(16, 6))\n",
    "    \n",
    "    for i in range(8):\n",
    "        # Original\n",
    "        axs[0, i].imshow(test_images[i].cpu().squeeze(), cmap='gray')\n",
    "        axs[0, i].set_title(f\"{class_names[test_labels[i]]}\")\n",
    "        axs[0, i].axis('off')\n",
    "        \n",
    "        # Reconstructed\n",
    "        axs[1, i].imshow(reconstructed_images[i].cpu().squeeze(), cmap='gray')\n",
    "        axs[1, i].set_title(\"Reconstructed\")\n",
    "        axs[1, i].axis('off')\n",
    "        \n",
    "        # Generated\n",
    "        axs[2, i].imshow(generated_images[i].cpu().squeeze(), cmap='gray')\n",
    "        axs[2, i].set_title(\"Generated\")\n",
    "        axs[2, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"evaluation_results.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    return vae, siren\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    vae, siren = train(num_epochs=1, save_interval=1)\n",
    "    evaluate_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEED TO REDUCE WEIGHTS OUTPUT OF VAE, NEED TOO FIX SIREN INIT, NEED TO FIX OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siren and VAE model (second implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data Loading \n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, transform=transform)\n",
    "test_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, train=False, transform=transform)\n",
    "\n",
    "batch_size = 16  \n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n",
    "\n",
    "def positional_encoding(coords, num_bands=8):\n",
    "    pos_enc = [coords]\n",
    "    for i in range(num_bands):\n",
    "        freq = 2.0 ** i\n",
    "        pos_enc.append(torch.sin(coords * freq * np.pi))\n",
    "        pos_enc.append(torch.cos(coords * freq * np.pi))\n",
    "    return torch.cat(pos_enc, dim=-1)\n",
    "\n",
    "def get_mgrid(sidelen, dim=2, num_bands=8):\n",
    "    if isinstance(sidelen, int):\n",
    "        sidelen = dim * (sidelen,)\n",
    "    coords = [torch.linspace(-1, 1, s) for s in sidelen]\n",
    "    mesh_coords = torch.meshgrid(*coords, indexing='ij')  \n",
    "    coords = torch.stack(mesh_coords, dim=-1).reshape(-1, dim)\n",
    "    return positional_encoding(coords, num_bands)\n",
    "\n",
    "class SIREN(nn.Module):\n",
    "    def __init__(self, input_dim=2+2*2*8, hidden_dim=256, hidden_layers=3, output_dim=1, first_omega=1.0, hidden_omega=10.0):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.first_omega = first_omega\n",
    "        self.hidden_omega = hidden_omega\n",
    "\n",
    "        self.first_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        with torch.no_grad():\n",
    "            self.first_layer.weight.uniform_(-1/input_dim, 1/input_dim)\n",
    "            self.first_layer.bias.uniform_(-1/input_dim, 1/input_dim)\n",
    "        \n",
    "        self.hidden_layers_list = nn.ModuleList()\n",
    "        for _ in range(hidden_layers):\n",
    "            layer = nn.Linear(hidden_dim, hidden_dim)\n",
    "            with torch.no_grad():\n",
    "                layer.weight.uniform_(-np.sqrt(6/hidden_dim)/hidden_omega, \n",
    "                                      np.sqrt(6/hidden_dim)/hidden_omega)\n",
    "                layer.bias.uniform_(-np.sqrt(6/hidden_dim)/hidden_omega, \n",
    "                                    np.sqrt(6/hidden_dim)/hidden_omega)\n",
    "            self.hidden_layers_list.append(layer)\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        with torch.no_grad():\n",
    "            self.output_layer.weight.uniform_(-np.sqrt(6/hidden_dim)/hidden_omega, \n",
    "                                              np.sqrt(6/hidden_dim)/hidden_omega)\n",
    "            self.output_layer.bias.uniform_(-1, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.sin(self.first_omega * self.first_layer(x))\n",
    "        for layer in self.hidden_layers_list:\n",
    "            x = torch.sin(self.hidden_omega * layer(x))\n",
    "\n",
    "        return torch.sigmoid(self.output_layer(x))\n",
    "    \n",
    "    def set_weights_vector(self, weights_vector):\n",
    "        \"\"\"Set all weights from a flattened vector\"\"\"\n",
    "        start_idx = 0\n",
    "        \n",
    "        w_size = self.first_layer.weight.numel()\n",
    "        self.first_layer.weight.data = weights_vector[start_idx:start_idx+w_size].view(self.first_layer.weight.shape)\n",
    "        start_idx += w_size\n",
    "        \n",
    "        b_size = self.first_layer.bias.numel()\n",
    "        self.first_layer.bias.data = weights_vector[start_idx:start_idx+b_size].view(self.first_layer.bias.shape)\n",
    "        start_idx += b_size\n",
    "        \n",
    "        for layer in self.hidden_layers_list:\n",
    "            w_size = layer.weight.numel()\n",
    "            layer.weight.data = weights_vector[start_idx:start_idx+w_size].view(layer.weight.shape)\n",
    "            start_idx += w_size\n",
    "            \n",
    "            b_size = layer.bias.numel()\n",
    "            layer.bias.data = weights_vector[start_idx:start_idx+b_size].view(layer.bias.shape)\n",
    "            start_idx += b_size\n",
    "        \n",
    "        w_size = self.output_layer.weight.numel()\n",
    "        self.output_layer.weight.data = weights_vector[start_idx:start_idx+w_size].view(self.output_layer.weight.shape)\n",
    "        start_idx += w_size\n",
    "        \n",
    "        b_size = self.output_layer.bias.numel()\n",
    "        self.output_layer.bias.data = weights_vector[start_idx:start_idx+b_size].view(self.output_layer.bias.shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def calc_total_weights(input_dim, hidden_dim, hidden_layers, output_dim):\n",
    "        \"\"\"Calculate the total number of weights in the SIREN\"\"\"\n",
    "        total = input_dim * hidden_dim + hidden_dim  # First layer\n",
    "        total += hidden_layers * (hidden_dim * hidden_dim + hidden_dim)  # Hidden layers\n",
    "        total += hidden_dim * output_dim + output_dim  # Output layer\n",
    "        return total\n",
    "\n",
    "    def get_weights_vector(self):\n",
    "        \"\"\"Get all weights as a flattened vector\"\"\"\n",
    "        weights = []\n",
    "        weights.append(self.first_layer.weight.data.flatten())\n",
    "        weights.append(self.first_layer.bias.data.flatten())\n",
    "        \n",
    "        for layer in self.hidden_layers_list:\n",
    "            weights.append(layer.weight.data.flatten())\n",
    "            weights.append(layer.bias.data.flatten())\n",
    "        \n",
    "        weights.append(self.output_layer.weight.data.flatten())\n",
    "        weights.append(self.output_layer.bias.data.flatten())\n",
    "        \n",
    "        return torch.cat(weights, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=100, siren_config=None):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.siren_config = {\n",
    "            'input_dim': 2 + 2*2*8,\n",
    "            'hidden_dim': 128,\n",
    "            'hidden_layers': 3,\n",
    "            'output_dim': 1\n",
    "        }\n",
    "        \n",
    "        if siren_config is not None:\n",
    "            self.siren_config.update(siren_config)\n",
    "        \n",
    "        self.generator = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 28 * 28),\n",
    "            nn.Tanh()\n",
    "            #nn.Unflatten(1, (128, 7, 7)),\n",
    "            #nn.Flatten()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.generator(z)\n",
    "        return x.view(-1, 1, 28, 28)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.discriminator(x)\n",
    "\n",
    "# GAN Class (Main Model)\n",
    "class GAN(nn.Module):\n",
    "    def __init__(self, z_dim=100):\n",
    "        super(GAN, self).__init__()\n",
    "\n",
    "        self.generator = Generator(z_dim=z_dim)\n",
    "        self.discriminator = Discriminator()\n",
    "\n",
    "    def forward(self, z):\n",
    "        generated_image = self.generator(z)\n",
    "        return generated_image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VAE(nn.Module):\n",
    "#     def __init__(self, z_dim=64, siren_config=None):\n",
    "#         super().__init__()\n",
    "#         self.z_dim = z_dim\n",
    "        \n",
    "#         self.siren_config = {\n",
    "#             'input_dim': 2 + 2*2*8,  \n",
    "#             'hidden_dim': 128,       \n",
    "#             'hidden_layers': 3,       \n",
    "#             'output_dim': 1\n",
    "#         }\n",
    "        \n",
    "#         if siren_config is not None:\n",
    "#             self.siren_config.update(siren_config)\n",
    "        \n",
    "#         self.encoder = nn.Sequential(\n",
    "#             nn.Conv2d(1, 32, 4, stride=2, padding=1),             # 28x28 -> 14x14\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Conv2d(32, 64, 4, stride=2, padding=1),            # 14x14 -> 7x7\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Conv2d(64, 128, 3, stride=1, padding=1),           # 7x7 -> 7x7\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(128 * 7 * 7, 512),\n",
    "#             nn.LeakyReLU(0.2, inplace=True)\n",
    "#         )\n",
    "        \n",
    "#         self.fc_mu = nn.Linear(512, z_dim)\n",
    "#         self.fc_logvar = nn.Linear(512, z_dim)\n",
    "        \n",
    "#         total_weights = SIREN.calc_total_weights(\n",
    "#             input_dim=self.siren_config['input_dim'],\n",
    "#             hidden_dim=self.siren_config['hidden_dim'],\n",
    "#             hidden_layers=self.siren_config['hidden_layers'],\n",
    "#             output_dim=self.siren_config['output_dim']\n",
    "#         )\n",
    "        \n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(z_dim, 512),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Linear(512, 1024),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Linear(1024, total_weights) \n",
    "#         )\n",
    "    \n",
    "#     def encode(self, x):\n",
    "#         h = self.encoder(x)\n",
    "#         return self.fc_mu(h), self.fc_logvar(h)\n",
    "    \n",
    "#     def reparameterize(self, mu, logvar):\n",
    "#         logvar = torch.clamp(logvar, -4, 4)\n",
    "#         std = torch.exp(0.5 * logvar)\n",
    "#         eps = torch.randn_like(std)\n",
    "#         return mu + eps * std\n",
    "    \n",
    "#     def decode(self, z):\n",
    "#         return self.decoder(z)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         mu, logvar = self.encode(x)\n",
    "#         z = self.reparameterize(mu, logvar)\n",
    "#         return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/3750 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 3750/3750 [01:02<00:00, 60.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 - Loss: 0.1353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 625/625 [00:02<00:00, 240.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.2114\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x1 and 100x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 191\u001b[0m\n\u001b[1;32m    189\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(test_loader))\n\u001b[1;32m    190\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 191\u001b[0m weights_vector_batch, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mgan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m reconstructions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    193\u001b[0m batch_size_local \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 191\u001b[0m, in \u001b[0;36mGAN.forward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, z):\n\u001b[0;32m--> 191\u001b[0m     generated_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generated_image\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 160\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, z):\n\u001b[0;32m--> 160\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x1 and 100x256)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "gan = GAN(z_dim=100).to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer_g = optim.Adam(gan.generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_d = optim.Adam(gan.discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Create the grid (same as before)\n",
    "grid = get_mgrid(28, dim=2, num_bands=8).to(device)  \n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 1\n",
    "gan.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    for real_images, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        real_images = real_images.to(device)\n",
    "        batch_size = real_images.size(0)\n",
    "\n",
    "        # Create labels for real and fake images\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "        # Train the Discriminator\n",
    "        optimizer_d.zero_grad()\n",
    "\n",
    "        # Forward pass for real images\n",
    "        real_output = gan.discriminator(real_images)\n",
    "        real_loss = criterion(real_output, real_labels)\n",
    "\n",
    "        # Generate fake images\n",
    "        z = torch.randn(batch_size, 100).to(device)\n",
    "        fake_images = gan.generator(z)\n",
    "\n",
    "        # Forward pass for fake images\n",
    "        fake_output = gan.discriminator(fake_images.detach())  # Detach to avoid backprop through the generator\n",
    "        fake_loss = criterion(fake_output, fake_labels)\n",
    "\n",
    "        # Combine real and fake losses\n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_loss.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # Train the Generator\n",
    "        optimizer_g.zero_grad()\n",
    "\n",
    "        # Forward pass for fake images (again, but this time calculate for generator's loss)\n",
    "        fake_output = gan.discriminator(fake_images)\n",
    "        g_loss = criterion(fake_output, real_labels)  # We want the generator to fool the discriminator\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "        # Update total train loss\n",
    "        train_loss += d_loss.item() + g_loss.item()\n",
    "\n",
    "    # Print loss for this epoch\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {train_loss:.4f}\")\n",
    "\n",
    "# Testing Loop\n",
    "gan.eval()\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, _ in tqdm(test_loader, desc=\"Testing\"):\n",
    "        images = images.to(device)\n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        # Labels for real and fake images\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "        # Forward pass for real images\n",
    "        real_output = gan.discriminator(images)\n",
    "        real_loss = criterion(real_output, real_labels)\n",
    "\n",
    "        # Generate fake images\n",
    "        z = torch.randn(batch_size, 100).to(device)\n",
    "        fake_images = gan.generator(z)\n",
    "\n",
    "        # Forward pass for fake images\n",
    "        fake_output = gan.discriminator(fake_images)\n",
    "        fake_loss = criterion(fake_output, fake_labels)\n",
    "\n",
    "        # Combine real and fake losses\n",
    "        d_loss = real_loss + fake_loss\n",
    "\n",
    "        # Update total test loss\n",
    "        test_loss += d_loss.item() * batch_size\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# vae = VAE(z_dim=64).to(device)\n",
    "\n",
    "# siren = SIREN(input_dim=vae.siren_config['input_dim'],\n",
    "#             hidden_dim=vae.siren_config['hidden_dim'],\n",
    "#             hidden_layers=vae.siren_config['hidden_layers'],\n",
    "#             output_dim=vae.siren_config['output_dim']).to(device)\n",
    "\n",
    "# optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "# bce_loss = nn.BCELoss()\n",
    "\n",
    "# grid = get_mgrid(28, dim=2, num_bands=8).to(device)  \n",
    "\n",
    "# num_epochs = 1\n",
    "# vae.train()\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_loss = 0.0\n",
    "#     for images, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "#         images = images.to(device)   # images shape: (batch, 1, 28, 28)\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         weights_vector_batch, mu, logvar = vae(images)\n",
    "        \n",
    "#         reconstructions = []\n",
    "#         batch_size_local = images.shape[0]\n",
    "#         for i in range(batch_size_local):\n",
    "#             with torch.no_grad():\n",
    "#                 siren.set_weights_vector(weights_vector_batch[i])\n",
    "#             rec = siren(grid) \n",
    "#             rec = rec.view(1, 28, 28) \n",
    "#             reconstructions.append(rec)\n",
    "\n",
    "#         reconstructions = torch.cat(reconstructions, dim=0)\n",
    "\n",
    "#         rec_loss = bce_loss(reconstructions, images.squeeze(1))\n",
    "\n",
    "#         kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        \n",
    "#         loss = rec_loss + kl_loss\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         train_loss += loss.item() * batch_size_local\n",
    "        \n",
    "#     train_loss /= len(train_loader.dataset)\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {train_loss:.4f}\")\n",
    "\n",
    "# vae.eval()\n",
    "# test_loss = 0.0\n",
    "# with torch.no_grad():\n",
    "#     for images, _ in tqdm(test_loader, desc=\"Testing\"):\n",
    "#         images = images.to(device)\n",
    "#         weights_vector_batch, mu, logvar = vae(images)\n",
    "#         reconstructions = []\n",
    "#         batch_size_local = images.shape[0]\n",
    "#         for i in range(batch_size_local):\n",
    "#             siren.set_weights_vector(weights_vector_batch[i])\n",
    "#             rec = siren(grid)  # output shape: (28*28, 1)\n",
    "#             rec = rec.view(1, 28, 28)\n",
    "#             reconstructions.append(rec)\n",
    "#         reconstructions = torch.cat(reconstructions, dim=0)\n",
    "#         rec_loss = bce_loss(reconstructions, images.squeeze(1))\n",
    "#         kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "#         loss = rec_loss + kl_loss\n",
    "#         test_loss += loss.item() * batch_size_local\n",
    "#     test_loss /= len(test_loader.dataset)\n",
    "# print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# # Visualize some reconstructed images.\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# vae.eval()\n",
    "# with torch.no_grad():\n",
    "    # images, labels = next(iter(test_loader))\n",
    "    # images = images.to(device)\n",
    "    # weights_vector_batch, _, _ = vae(images)\n",
    "    # reconstructions = []\n",
    "    # batch_size_local = images.shape[0]\n",
    "    # for i in range(batch_size_local):\n",
    "    #     siren.set_weights_vector(weights_vector_batch[i])\n",
    "    #     rec = siren(grid)\n",
    "    #     rec = rec.view(28, 28)\n",
    "    #     reconstructions.append(rec.cpu().numpy())\n",
    "\n",
    "gan.eval()\n",
    "with torch.no_grad():\n",
    "    images, labels = next(iter(test_loader))\n",
    "    images = images.to(device)\n",
    "    weights_vector_batch, _, _ = gan(real_output)\n",
    "    reconstructions = []\n",
    "    batch_size_local = images.shape[0]\n",
    "    for i in range(batch_size_local):\n",
    "        siren.set_weights_vector(weights_vector_batch[i])\n",
    "        rec = siren(grid)\n",
    "        rec = rec.view(28, 28)\n",
    "        reconstructions.append(rec.cpu().numpy())\n",
    "    # z = torch.randn(16, 100).to(device)  # Generate 16 random latent vectors\n",
    "    # fake_images = gan.generator(z)\n",
    "\n",
    "    # # Rescale and display the generated images\n",
    "    # fake_images = fake_images.cpu().detach()\n",
    "    # grid_img = torchvision.utils.make_grid(fake_images, nrow=4, normalize=True)\n",
    "    # plt.imshow(grid_img.permute(1, 2, 0))  # Convert to HWC format\n",
    "    # plt.axis('off')\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "num_examples = 6\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i in range(num_examples):\n",
    "\n",
    "    plt.subplot(2, num_examples, i+1)\n",
    "    plt.imshow(images[i].cpu().squeeze(0), cmap=\"gray\")\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(2, num_examples, num_examples+i+1)\n",
    "    plt.imshow(reconstructions[i], cmap=\"gray\")\n",
    "    plt.title(\"Reconstruction\")\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/3750 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 3750/3750 [01:01<00:00, 61.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 - Loss: 0.1369\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_examples):\n\u001b[1;32m     83\u001b[0m     plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m2\u001b[39m, num_examples, num_examples \u001b[38;5;241m+\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_images\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgray\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m     plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/matplotlib/pyplot.py:3562\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   3541\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mimshow)\n\u001b[1;32m   3542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimshow\u001b[39m(\n\u001b[1;32m   3543\u001b[0m     X: ArrayLike \u001b[38;5;241m|\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3560\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3561\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AxesImage:\n\u001b[0;32m-> 3562\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3566\u001b[0m \u001b[43m        \u001b[49m\u001b[43maspect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maspect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3567\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3568\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3571\u001b[0m \u001b[43m        \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3573\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilternorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilterrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3577\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3578\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3579\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3580\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3581\u001b[0m     sci(__ret)\n\u001b[1;32m   3582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/matplotlib/__init__.py:1476\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1475\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1476\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1477\u001b[0m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1478\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1479\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1481\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1482\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1483\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/matplotlib/axes/_axes.py:5895\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5893\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m-> 5895\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5896\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5898\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/matplotlib/image.py:729\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m    728\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/matplotlib/image.py:690\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_normalize_image_array\u001b[39m(A):\n\u001b[1;32m    686\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;124;03m    Check validity of image-like input *A* and normalize it to a format suitable for\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;124;03m    Image subclasses.\u001b[39;00m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 690\u001b[0m     A \u001b[38;5;241m=\u001b[39m \u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_masked_invalid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39muint8 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mcan_cast(A\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame_kind\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    692\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage data of dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    693\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconverted to float\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/matplotlib/cbook.py:733\u001b[0m, in \u001b[0;36msafe_masked_invalid\u001b[0;34m(x, copy)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msafe_masked_invalid\u001b[39m(x, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 733\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39misnative:\n\u001b[1;32m    735\u001b[0m         \u001b[38;5;66;03m# If we have already made a copy, do the byteswap in place, else make a\u001b[39;00m\n\u001b[1;32m    736\u001b[0m         \u001b[38;5;66;03m# copy with the byte order swapped.\u001b[39;00m\n\u001b[1;32m    737\u001b[0m         \u001b[38;5;66;03m# Swap to native order.\u001b[39;00m\n\u001b[1;32m    738\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mbyteswap(inplace\u001b[38;5;241m=\u001b[39mcopy)\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mnewbyteorder(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/_tensor.py:1194\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 7 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "gan = GAN(z_dim=100).to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer_g = optim.Adam(gan.generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_d = optim.Adam(gan.discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Create the grid (same as before)\n",
    "grid = get_mgrid(28, dim=2, num_bands=8).to(device)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 1\n",
    "gan.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    for real_images, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        real_images = real_images.to(device)\n",
    "        batch_size = real_images.size(0)\n",
    "\n",
    "        # Create labels for real and fake images\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "        # Train the Discriminator\n",
    "        optimizer_d.zero_grad()\n",
    "\n",
    "        # Forward pass for real images\n",
    "        real_output = gan.discriminator(real_images)\n",
    "        real_loss = criterion(real_output, real_labels)\n",
    "\n",
    "        # Generate fake images\n",
    "        z = torch.randn(batch_size, 100).to(device)\n",
    "        fake_images = gan.generator(z)\n",
    "\n",
    "        # Forward pass for fake images\n",
    "        fake_output = gan.discriminator(fake_images.detach())  # Detach to avoid backprop through the generator\n",
    "        fake_loss = criterion(fake_output, fake_labels)\n",
    "\n",
    "        # Combine real and fake losses\n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_loss.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # Train the Generator\n",
    "        optimizer_g.zero_grad()\n",
    "\n",
    "        # Forward pass for fake images (again, but this time calculate for generator's loss)\n",
    "        fake_output = gan.discriminator(fake_images)\n",
    "        g_loss = criterion(fake_output, real_labels)  # We want the generator to fool the discriminator\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "        # Update total train loss\n",
    "        train_loss += d_loss.item() + g_loss.item()\n",
    "\n",
    "    # Print loss for this epoch\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Visualizing images every epoch (you can adjust to every few batches if needed)\n",
    "    if epoch % 1 == 0:  # Adjust for every epoch\n",
    "        num_examples = 6\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        # Plot real images\n",
    "        for i in range(num_examples):\n",
    "            plt.subplot(2, num_examples, i+1)\n",
    "            plt.imshow(real_images[i].cpu().squeeze(0), cmap=\"gray\")\n",
    "            plt.title(\"Real\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "        # Plot generated (fake) images\n",
    "        for i in range(num_examples):\n",
    "            plt.subplot(2, num_examples, num_examples + i + 1)\n",
    "            plt.imshow(fake_images[i].cpu().squeeze(0), cmap=\"gray\")\n",
    "            plt.title(\"Generated\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Testing Loop\n",
    "gan.eval()\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, _ in tqdm(test_loader, desc=\"Testing\"):\n",
    "        images = images.to(device)\n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        # Labels for real and fake images\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "        # Forward pass for real images\n",
    "        real_output = gan.discriminator(images)\n",
    "        real_loss = criterion(real_output, real_labels)\n",
    "\n",
    "        # Generate fake images\n",
    "        z = torch.randn(batch_size, 100).to(device)\n",
    "        fake_images = gan.generator(z)\n",
    "\n",
    "        # Forward pass for fake images\n",
    "        fake_output = gan.discriminator(fake_images)\n",
    "        fake_loss = criterion(fake_output, fake_labels)\n",
    "\n",
    "        # Combine real and fake losses\n",
    "        d_loss = real_loss + fake_loss\n",
    "\n",
    "        # Update total test loss\n",
    "        test_loss += d_loss.item() * batch_size\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# After training, visualize some generated images\n",
    "num_examples = 6\n",
    "z = torch.randn(num_examples, 100).to(device)  # Generate random latent vectors\n",
    "generated_images = gan.generator(z)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i in range(num_examples):\n",
    "    plt.subplot(2, num_examples, num_examples + i + 1)\n",
    "    plt.imshow(fake_images[i].cpu().detach().squeeze(0).numpy(), cmap=\"gray\")\n",
    "    plt.title(\"Generated\")\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
