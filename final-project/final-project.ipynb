{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBAL VARIABLES\n",
    "OMEGA_0 = 30.0\n",
    "HIDDEN_SIZE = 128 \n",
    "N_HIDDEN_LAYERS = 3  \n",
    "IMAGE_CHANNELS = 1\n",
    "Z_DIM = 64\n",
    "BATCH_SIZE = 64  \n",
    "LEARNING_RATE = 1e-3  \n",
    "EPOCHS = 25\n",
    "NUM_BANDS = 8  \n",
    "IMG_SIZE = 28 \n",
    "NUM_WORKERS = 4  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FASTER TRAINING\n",
    "try:\n",
    "    if hasattr(torch, 'backends') and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        NUM_WORKERS = 1  \n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(coords, num_bands=NUM_BANDS):\n",
    "    \"\"\"Apply positional encoding to the input coordinates.\"\"\"\n",
    "    pos_enc = [coords]\n",
    "    for i in range(num_bands):\n",
    "        freq = 2.0 ** i\n",
    "        pos_enc.append(torch.sin(coords * freq * np.pi))\n",
    "        pos_enc.append(torch.cos(coords * freq * np.pi))\n",
    "    return torch.cat(pos_enc, dim=-1)\n",
    "\n",
    "def get_mgrid(sidelen, dim=2, num_bands=NUM_BANDS):\n",
    "    \"\"\"Get a meshgrid of coordinates with positional encoding.\"\"\"\n",
    "    if isinstance(sidelen, int):\n",
    "        sidelen = dim * (sidelen,)\n",
    "    coords = [torch.linspace(-1, 1, s) for s in sidelen]\n",
    "    mesh_coords = torch.meshgrid(*coords, indexing='ij')  \n",
    "    coords = torch.stack(mesh_coords, dim=-1).reshape(-1, dim)\n",
    "    return positional_encoding(coords, num_bands)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SIREN\n",
    "class SineLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, omega_0=OMEGA_0):\n",
    "        super().__init__()\n",
    "        self.omega_0 = omega_0\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "    \n",
    "    def forward(self, input, W, b):\n",
    "        return torch.sin(self.omega_0 * F.linear(input, W, b))\n",
    "\n",
    "class SIREN(nn.Module):\n",
    "    def __init__(self, input_dim=2+2*2*NUM_BANDS, hidden_dim=HIDDEN_SIZE, \n",
    "                hidden_layers=N_HIDDEN_LAYERS, output_dim=IMAGE_CHANNELS):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Calculate sizes for weight partitioning\n",
    "        self.weight_sizes = self.calc_weight_sizes()\n",
    "        self.total_weights = sum(self.weight_sizes)\n",
    "        \n",
    "        # Create layers\n",
    "        self.first_layer = SineLayer(input_dim, hidden_dim, omega_0=30.0)\n",
    "        self.hidden_layers_list = nn.ModuleList([\n",
    "            SineLayer(hidden_dim, hidden_dim) for _ in range(hidden_layers)\n",
    "        ])\n",
    "    \n",
    "    def calc_weight_sizes(self):\n",
    "        \"\"\"Calculate sizes of each weight tensor for partitioning.\"\"\"\n",
    "        sizes = []\n",
    "        # First layer weights and biases\n",
    "        sizes.append(self.input_dim * self.hidden_dim)  # W1\n",
    "        sizes.append(self.hidden_dim)  # b1\n",
    "        \n",
    "        # Hidden layers weights and biases\n",
    "        for _ in range(self.hidden_layers):\n",
    "            sizes.append(self.hidden_dim * self.hidden_dim)  # W\n",
    "            sizes.append(self.hidden_dim)  # b\n",
    "        \n",
    "        # Output layer weights and biases\n",
    "        sizes.append(self.hidden_dim * self.output_dim)  # W_out\n",
    "        sizes.append(self.output_dim)  # b_out\n",
    "        \n",
    "        return sizes\n",
    "    \n",
    "    @staticmethod\n",
    "    def calc_total_weights(input_dim, hidden_dim, hidden_layers, output_dim):\n",
    "        \"\"\"Calculate total number of weights needed for the network.\"\"\"\n",
    "        total = input_dim * hidden_dim + hidden_dim  # First layer\n",
    "        total += hidden_layers * (hidden_dim * hidden_dim + hidden_dim)  # Hidden layers\n",
    "        total += hidden_dim * output_dim + output_dim  # Output layer\n",
    "        return total\n",
    "    \n",
    "    def forward(self, coords, weights):\n",
    "        \"\"\"Forward pass using dynamically generated weights.\"\"\"\n",
    "        # Split weights into layer-specific parts\n",
    "        weight_parts = []\n",
    "        start_idx = 0\n",
    "        for size in self.weight_sizes:\n",
    "            weight_parts.append(weights[:, start_idx:start_idx+size])\n",
    "            start_idx += size\n",
    "        \n",
    "        # Apply first layer\n",
    "        idx = 0\n",
    "        W1 = weight_parts[idx].view(-1, self.hidden_dim, self.input_dim)\n",
    "        idx += 1\n",
    "        b1 = weight_parts[idx]\n",
    "        idx += 1\n",
    "        \n",
    "        # Process each example in the batch separately\n",
    "        batch_size = weights.shape[0]\n",
    "        outputs = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            x = self.first_layer(coords, W1[i], b1[i])\n",
    "            \n",
    "            # Apply hidden layers\n",
    "            for layer_idx, layer in enumerate(self.hidden_layers_list):\n",
    "                W = weight_parts[idx + layer_idx*2].view(-1, self.hidden_dim, self.hidden_dim)[i]\n",
    "                b = weight_parts[idx + layer_idx*2 + 1][i]\n",
    "                x = layer(x, W, b)\n",
    "            \n",
    "            # Apply final layer\n",
    "            final_idx = idx + len(self.hidden_layers_list) * 2\n",
    "            W_final = weight_parts[final_idx].view(-1, self.output_dim, self.hidden_dim)[i]\n",
    "            b_final = weight_parts[final_idx + 1][i]\n",
    "            x = F.linear(x, W_final, b_final)\n",
    "\n",
    "            x = torch.sigmoid(x * 1.5) \n",
    "            \n",
    "            outputs.append(x)\n",
    "        \n",
    "        # Stack all batch outputs\n",
    "        return torch.stack(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VAE\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, z_dim=Z_DIM, siren_config=None):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "        # Default SIREN config\n",
    "        self.siren_config = {\n",
    "            'input_dim': 2 + 2*2*NUM_BANDS,  # 2D coords + positional encoding\n",
    "            'hidden_dim': HIDDEN_SIZE,       \n",
    "            'hidden_layers': N_HIDDEN_LAYERS,       \n",
    "            'output_dim': IMAGE_CHANNELS\n",
    "        }\n",
    "        \n",
    "        # Update config if provided\n",
    "        if siren_config is not None:\n",
    "            self.siren_config.update(siren_config)\n",
    "        \n",
    "        # Create the SIREN to use for decoding\n",
    "        self.siren = SIREN(\n",
    "            input_dim=self.siren_config['input_dim'],\n",
    "            hidden_dim=self.siren_config['hidden_dim'],\n",
    "            hidden_layers=self.siren_config['hidden_layers'],\n",
    "            output_dim=self.siren_config['output_dim']\n",
    "        )\n",
    "        \n",
    "        # Simplified Encoder network\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(IMAGE_CHANNELS, 32, 4, stride=2, padding=1),  # 28x28 -> 14x14\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(32, 64, 4, stride=2, padding=1),              # 14x14 -> 7x7\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Latent space parameters\n",
    "        self.fc_mu = nn.Linear(256, z_dim)\n",
    "        self.fc_logvar = nn.Linear(256, z_dim)\n",
    "        \n",
    "        # Calculate total weights needed for SIREN\n",
    "        total_weights = SIREN.calc_total_weights(\n",
    "            input_dim=self.siren_config['input_dim'],\n",
    "            hidden_dim=self.siren_config['hidden_dim'],\n",
    "            hidden_layers=self.siren_config['hidden_layers'],\n",
    "            output_dim=self.siren_config['output_dim']\n",
    "        )\n",
    "        \n",
    "        # Simplified Decoder network (produces weights for SIREN)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, total_weights) \n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode input images to latent space parameters.\"\"\"\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Reparameterization trick to sample from latent space.\"\"\"\n",
    "        logvar = torch.clamp(logvar, -4, 4)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode latent vectors to SIREN weights.\"\"\"\n",
    "        raw_weights = self.decoder(z)\n",
    "        \n",
    "        # Split and scale weights for different SIREN layers\n",
    "        weights_list = []\n",
    "        start_idx = 0\n",
    "        \n",
    "        # First layer weights - needs uniform(-1/in_dim, 1/in_dim)\n",
    "        in_dim = self.siren_config['input_dim']\n",
    "        first_w_size = in_dim * self.siren_config['hidden_dim']\n",
    "        first_b_size = self.siren_config['hidden_dim']\n",
    "        \n",
    "        first_w = raw_weights[:, start_idx:start_idx+first_w_size].view(-1, self.siren_config['hidden_dim'], in_dim)\n",
    "        start_idx += first_w_size\n",
    "        first_w = torch.tanh(first_w) * (1.0/in_dim)  # Scale to proper range\n",
    "        weights_list.append(first_w.reshape(-1, first_w_size))\n",
    "        \n",
    "        first_b = raw_weights[:, start_idx:start_idx+first_b_size]\n",
    "        start_idx += first_b_size\n",
    "        first_b = torch.tanh(first_b) * (1.0/in_dim)  # Scale to proper range\n",
    "        weights_list.append(first_b)\n",
    "        \n",
    "        # Hidden layers - needs scaled by sqrt(6/hidden_dim)/omega\n",
    "        hidden_dim = self.siren_config['hidden_dim']\n",
    "        hidden_omega = 30.0\n",
    "        scale_factor = np.sqrt(6/hidden_dim)/hidden_omega\n",
    "        \n",
    "        for _ in range(self.siren_config['hidden_layers']):\n",
    "            hidden_w_size = hidden_dim * hidden_dim\n",
    "            hidden_w = raw_weights[:, start_idx:start_idx+hidden_w_size].view(-1, hidden_dim, hidden_dim)\n",
    "            start_idx += hidden_w_size\n",
    "            hidden_w = torch.tanh(hidden_w) * scale_factor\n",
    "            weights_list.append(hidden_w.reshape(-1, hidden_w_size))\n",
    "            \n",
    "            hidden_b_size = hidden_dim\n",
    "            hidden_b = raw_weights[:, start_idx:start_idx+hidden_b_size]\n",
    "            start_idx += hidden_b_size\n",
    "            hidden_b = torch.tanh(hidden_b) * scale_factor\n",
    "            weights_list.append(hidden_b)\n",
    "        \n",
    "        # Output layer\n",
    "        out_dim = self.siren_config['output_dim']\n",
    "        out_w_size = hidden_dim * out_dim\n",
    "        out_w = raw_weights[:, start_idx:start_idx+out_w_size].view(-1, out_dim, hidden_dim)\n",
    "        start_idx += out_w_size\n",
    "        out_w = torch.tanh(out_w) * scale_factor\n",
    "        weights_list.append(out_w.reshape(-1, out_w_size))\n",
    "        \n",
    "        out_b_size = out_dim\n",
    "        out_b = raw_weights[:, start_idx:start_idx+out_b_size]\n",
    "        out_b = torch.tanh(out_b)  # Range (-1, 1) is fine for output bias\n",
    "        weights_list.append(out_b)\n",
    "        \n",
    "        # Concatenate all scaled weights\n",
    "        return torch.cat(weights_list, dim=1)\n",
    "    \n",
    "    def forward(self, x, coords=None, img_size=IMG_SIZE):\n",
    "        \"\"\"Forward pass through the VAE.\"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        siren_weights = self.decode(z)\n",
    "        \n",
    "        # Generate coordinate grid if not provided\n",
    "        if coords is None:\n",
    "            coords = get_mgrid(img_size, dim=2, num_bands=NUM_BANDS).to(x.device)\n",
    "        \n",
    "        # Get image reconstruction from SIREN\n",
    "        batch_recons = self.siren(coords, siren_weights)\n",
    "        \n",
    "        # Reshape to proper image format [B, C, H, W]\n",
    "        batch_recons = batch_recons.reshape(z.size(0), IMAGE_CHANNELS, img_size, img_size)\n",
    "        \n",
    "        return batch_recons, mu, logvar, siren_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar, kld_weight=0.001):\n",
    "    \"\"\"VAE loss function combining reconstruction and KL divergence.\"\"\"\n",
    "    # Reconstruction loss (MSE)\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    \n",
    "    # KL divergence\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    # Total loss\n",
    "    return recon_loss + kld_weight * kl_loss, recon_loss, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, device, epoch, img_size=IMG_SIZE, kld_weight=0.001):\n",
    "    \"\"\"Train the model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    recon_loss_total = 0\n",
    "    kl_loss_total = 0\n",
    "    \n",
    "    # Generate coordinate grid once\n",
    "    coords = get_mgrid(img_size, dim=2, num_bands=NUM_BANDS).to(device)\n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        recon_batch, mu, logvar, _ = model(data, coords, img_size)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss, recon_loss, kl_loss = loss_function(recon_batch, data, mu, logvar, kld_weight)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track loss\n",
    "        train_loss += loss.item()\n",
    "        recon_loss_total += recon_loss.item()\n",
    "        kl_loss_total += kl_loss.item()\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\t'\n",
    "                  f'Loss: {loss.item() / len(data):.6f}')\n",
    "    \n",
    "    # Average losses\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    avg_recon = recon_loss_total / len(train_loader.dataset)\n",
    "    avg_kl = kl_loss_total / len(train_loader.dataset)\n",
    "    \n",
    "    print(f'====> Epoch: {epoch} Average loss: {avg_loss:.6f} '\n",
    "          f'(Recon: {avg_recon:.6f}, KL: {avg_kl:.6f})')\n",
    "    \n",
    "    return avg_loss, avg_recon, avg_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, device, epoch, img_size=IMG_SIZE, kld_weight=0.001):\n",
    "    \"\"\"Evaluate the model on test data.\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    recon_loss_total = 0\n",
    "    kl_loss_total = 0\n",
    "    \n",
    "    # Generate coordinate grid once\n",
    "    coords = get_mgrid(img_size, dim=2, num_bands=NUM_BANDS).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            recon_batch, mu, logvar, _ = model(data, coords, img_size)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss, recon_loss, kl_loss = loss_function(recon_batch, data, mu, logvar, kld_weight)\n",
    "            \n",
    "            # Track loss\n",
    "            test_loss += loss.item()\n",
    "            recon_loss_total += recon_loss.item()\n",
    "            kl_loss_total += kl_loss.item()\n",
    "    \n",
    "    # Average losses\n",
    "    avg_loss = test_loss / len(test_loader.dataset)\n",
    "    avg_recon = recon_loss_total / len(test_loader.dataset)\n",
    "    avg_kl = kl_loss_total / len(test_loader.dataset)\n",
    "    \n",
    "    print(f'====> Test set loss: {avg_loss:.6f} '\n",
    "          f'(Recon: {avg_recon:.6f}, KL: {avg_kl:.6f})')\n",
    "    \n",
    "    return avg_loss, avg_recon, avg_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_reconstructions(model, data_loader, device, num_samples=8, img_size=IMG_SIZE):\n",
    "    \"\"\"Visualize original and reconstructed images.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch of test images\n",
    "    for data, _ in data_loader:\n",
    "        data = data.to(device)\n",
    "        break\n",
    "    \n",
    "    # Select a subset of images\n",
    "    data = data[:num_samples]\n",
    "    \n",
    "    # Generate coordinate grid\n",
    "    coords = get_mgrid(img_size, dim=2, num_bands=NUM_BANDS).to(device)\n",
    "    \n",
    "    # Get reconstructions\n",
    "    with torch.no_grad():\n",
    "        recon_batch, _, _, _ = model(data, coords, img_size)\n",
    "        \n",
    "        # Apply additional post-processing for visualization\n",
    "        # This enhances contrast without affecting the training\n",
    "        recon_batch = torch.clamp(recon_batch * 1.2, 0, 1)\n",
    "    \n",
    "    # Concatenate original and reconstructed images\n",
    "    comparison = torch.cat([data, recon_batch])\n",
    "    \n",
    "    # Create a grid of images\n",
    "    grid = make_grid(comparison, nrow=num_samples)\n",
    "    \n",
    "    # Convert to numpy and transpose for plotting\n",
    "    grid_np = grid.cpu().numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(grid_np, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title('Original (top) vs Reconstructed (bottom)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Enable CUDA, MPS (Apple Silicon), XPU (Intel GPU), or fallback to CPU\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    elif hasattr(torch, 'backends') and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')  # Apple Silicon support\n",
    "    elif hasattr(torch, 'xpu') and torch.xpu.is_available():  # Intel GPU support\n",
    "        device = torch.device('xpu')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load Fashion MNIST dataset with subset of training data\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    full_train_dataset = datasets.FashionMNIST('./data', train=True, download=True, transform=transform)\n",
    "\n",
    "    subset_size = int(len(full_train_dataset) * 0.3)\n",
    "    indices = torch.randperm(len(full_train_dataset))[:subset_size]\n",
    "    train_dataset = torch.utils.data.Subset(full_train_dataset, indices)\n",
    "    test_dataset = datasets.FashionMNIST('./data', train=False, download=True, transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    model = VAE().to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=1, factor=0.5)\n",
    "    \n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        # Train\n",
    "        train_loss, train_recon, train_kl = train_model(model, train_loader, optimizer, device, epoch)\n",
    "        train_losses.append((train_loss, train_recon, train_kl))\n",
    "        \n",
    "        # Test\n",
    "        test_loss, test_recon, test_kl = test_model(model, test_loader, device, epoch)\n",
    "        test_losses.append((test_loss, test_recon, test_kl))\n",
    "        \n",
    "        # Update learning rate based on test loss\n",
    "        scheduler.step(test_loss)\n",
    "        \n",
    "        # Visualize reconstructions every epoch\n",
    "        visualize_reconstructions(model, test_loader, device)\n",
    "    \n",
    "    # Plot training and test loss\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot([x[0] for x in train_losses], label='Train')\n",
    "    plt.plot([x[0] for x in test_losses], label='Test')\n",
    "    plt.title('Total Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot([x[1] for x in train_losses], label='Train')\n",
    "    plt.plot([x[1] for x in test_losses], label='Test')\n",
    "    plt.title('Reconstruction Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot([x[2] for x in train_losses], label='Train')\n",
    "    plt.plot([x[2] for x in test_losses], label='Test')\n",
    "    plt.title('KL Divergence')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
